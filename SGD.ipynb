{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Group:\n",
    "\n",
    "-  Ali Janloo\n",
    "- Ali Alavizadeh\n",
    "- Amir Ali Akhgari"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent (SGD) Algorithm Explanation\n",
    "\n",
    "## Background\n",
    "SGD is a method for minimizing an objective function, often encountered in statistical estimation and machine learning tasks. The objective function typically takes the form of a sum of functions, where each function represents the error associated with individual observations in the dataset.\n",
    "\n",
    "## Iterative Method\n",
    "1. **Initialization**: Choose initial parameters `w` and a learning rate `η`.\n",
    "2. **Iterative Updates**:\n",
    "   - **Shuffle Data**: Randomly shuffle the samples in the training set.\n",
    "   - **Gradient Descent Step**: For each observation in the training set:\n",
    "     ```\n",
    "     w := w - η ∇Qi(w)\n",
    "     ```\n",
    "     where `∇Qi(w)` represents the gradient of the objective function with respect to the parameters `w` computed for the `i-th` batch.\n",
    "3. **Convergence**: Repeat the iterative updates until an approximate minimum of the objective function is obtained.\n",
    "\n",
    "## Mini-batch SGD\n",
    "To strike a balance between the computational efficiency of batch gradient descent and the randomness of stochastic gradient descent, mini-batch SGD is often used. In this approach:\n",
    "- Instead of updating parameters based on a single observation, updates are computed based on a mini-batch of observations.\n",
    "- This can enhance computational efficiency by leveraging vectorization libraries and may lead to smoother convergence compared to pure stochastic updates.\n",
    "\n",
    "## Convergence Analysis\n",
    "The convergence of SGD has been analyzed in the context of convex minimization and stochastic approximation theories. Under certain conditions:\n",
    "- If the learning rate decreases appropriately and the objective function is convex or pseudoconvex, SGD converges almost surely to a global minimum.\n",
    "- Otherwise, it converges almost surely to a local minimum.\n",
    "\n",
    "## Example\n",
    "Suppose we want to fit a straight line to a dataset using least squares. The objective function to be minimized is the sum of squared errors between predicted and actual values.\n",
    "- SGD updates the parameters `w` iteratively based on the gradient of this objective function, where each update is influenced by a single observation in the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SGD:\n",
    "    def __init__(self, lr=0.01, epochs=10000, batch_size=32, tol=1e-3):\n",
    "        self.learning_rate = lr\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.tolerance = tol\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.dot(X, self.weights) + self.bias\n",
    "\n",
    "    def mean_squared_error(self, y_true, y_pred):\n",
    "        return np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "    def gradient(self, X_batch, y_batch):\n",
    "        y_pred = self.predict(X_batch)\n",
    "        error = y_pred - y_batch\n",
    "        gradient_weights = np.dot(X_batch.T, error) / X_batch.shape[0]\n",
    "        gradient_bias = np.mean(error)\n",
    "        return gradient_weights, gradient_bias\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.random.randn(n_features)\n",
    "        self.bias = np.random.randn()\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            indices = np.random.permutation(n_samples)\n",
    "            X_shuffled = X[indices]\n",
    "            y_shuffled = y[indices]\n",
    "\n",
    "            for i in range(0, n_samples, self.batch_size):\n",
    "                X_batch = X_shuffled[i:i+self.batch_size]\n",
    "                y_batch = y_shuffled[i:i+self.batch_size]\n",
    "\n",
    "                gradient_weights, gradient_bias = self.gradient(X_batch, y_batch)\n",
    "                self.weights -= self.learning_rate * gradient_weights  # update weights\n",
    "                self.bias -= self.learning_rate * gradient_bias\n",
    "\n",
    "            if epoch % 100 == 0:\n",
    "                y_pred = self.predict(X)\n",
    "                loss = self.mean_squared_error(y, y_pred)\n",
    "                print(f\"Epoch {epoch}: Loss {loss}\")\n",
    "\n",
    "            if np.linalg.norm(gradient_weights) < self.tolerance:\n",
    "                print(\"Convergence reached.\")\n",
    "                break\n",
    "\n",
    "        return self.weights, self.bias\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss 32.97801004543985\n",
      "Epoch 100: Loss 0.05165746154305697\n",
      "Epoch 200: Loss 0.007288659061701172\n",
      "Epoch 300: Loss 0.007093813261605927\n",
      "Epoch 400: Loss 0.0071046640282487125\n",
      "Epoch 500: Loss 0.0071002299810878685\n",
      "Epoch 600: Loss 0.0071069618409829345\n",
      "Epoch 700: Loss 0.007111344537908192\n",
      "Epoch 800: Loss 0.0070954469975072045\n",
      "Epoch 900: Loss 0.007100144825010096\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X = np.random.randn(100, 5)\n",
    "y = np.dot(X, np.array([1, 2, 3, 4, 5])) + np.random.randn(100) * 0.1\n",
    "\n",
    "model = SGD(lr=0.01, epochs=1000, batch_size=32, tol=1e-3)\n",
    "\n",
    "w, b = model.fit(X, y)\n",
    "y_pred = w * X + b"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
